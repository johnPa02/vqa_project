{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 10992551,
          "sourceType": "datasetVersion",
          "datasetId": 6842215
        },
        {
          "sourceId": 11019841,
          "sourceType": "datasetVersion",
          "datasetId": 6861822
        },
        {
          "sourceId": 11115721,
          "sourceType": "datasetVersion",
          "datasetId": 6930799
        },
        {
          "sourceId": 296089,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 253474,
          "modelId": 274923
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "vqa-main",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T08:50:37.020603Z",
          "iopub.execute_input": "2025-03-22T08:50:37.020921Z",
          "iopub.status.idle": "2025-03-22T08:50:40.04716Z",
          "shell.execute_reply.started": "2025-03-22T08:50:37.020896Z",
          "shell.execute_reply": "2025-03-22T08:50:40.046293Z"
        },
        "id": "iuKNqoI8KJ43"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard\n",
        "!pip install loguru"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T08:50:41.33145Z",
          "iopub.execute_input": "2025-03-22T08:50:41.331905Z",
          "iopub.status.idle": "2025-03-22T08:50:49.011112Z",
          "shell.execute_reply.started": "2025-03-22T08:50:41.331874Z",
          "shell.execute_reply": "2025-03-22T08:50:49.010127Z"
        },
        "id": "ou0AgsbSKJ44"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class VQADataset(Dataset):\n",
        "    def __init__(self, ques_h5, img_h5, json_file, split='train'):\n",
        "        self.h5_img = h5py.File(img_h5, 'r')\n",
        "        self.h5_ques = h5py.File(ques_h5, 'r')\n",
        "        self.json_data = json.load(open(json_file))\n",
        "        self.vocab_size = len(self.json_data['ix_to_word'])\n",
        "\n",
        "        dataset_key = 'train' if split == 'train' else 'test'\n",
        "        self.img_pos = self.h5_ques[f'img_pos_{dataset_key}'][:]\n",
        "        self.fv_ims = self.h5_img[f'images_{dataset_key}'][:]\n",
        "        self.ques = self.h5_ques[f'ques_{dataset_key}'][:]\n",
        "        self.ques_len = self.h5_ques[f'ques_len_{dataset_key}'][:]\n",
        "        self.ques_id = self.h5_ques[f'ques_id_{dataset_key}'][:]\n",
        "        if dataset_key == 'test':\n",
        "            self.answer = self.h5_ques[f'ans_{dataset_key}'][:]\n",
        "        else:\n",
        "            self.answer = self.h5_ques[f'answers'][:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ques_id)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'image': torch.tensor(self.fv_ims[self.img_pos[idx]], dtype=torch.float),\n",
        "            'question': torch.tensor(self.ques[idx], dtype=torch.long),\n",
        "            'question_len': torch.tensor(self.ques_len[idx], dtype=torch.long),\n",
        "            'answer': torch.tensor(self.answer[idx], dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T08:50:54.86053Z",
          "iopub.execute_input": "2025-03-22T08:50:54.860812Z",
          "iopub.status.idle": "2025-03-22T08:50:54.868548Z",
          "shell.execute_reply.started": "2025-03-22T08:50:54.860789Z",
          "shell.execute_reply": "2025-03-22T08:50:54.86761Z"
        },
        "id": "3pYfft5KKJ45"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline LSTM+CNN"
      ],
      "metadata": {
        "id": "1u-CRGEaKJ46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuestionEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, dropout=0.5):\n",
        "        super(QuestionEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.tanh = nn.Tanh()\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len)\n",
        "        x = self.embedding(x)         # (batch, seq_len, embedding_size)\n",
        "        x = self.dropout(x)\n",
        "        x = self.tanh(x)\n",
        "        return x\n",
        "\n",
        "class QuestionEncoder(nn.Module):\n",
        "    def __init__(self, embedding_size, lstm_size, num_layers, dropout=0.5):\n",
        "        super(QuestionEncoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_size,\n",
        "            hidden_size=lstm_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "    def forward(self, embeddings, lengths):\n",
        "        # Pack padded sequence for efficient processing\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embeddings, lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
        "        packed_out, (hn, cn) = self.lstm(packed)\n",
        "        # Use the hidden state from the last layer (hn: [num_layers, batch, lstm_size])\n",
        "        return hn[-1]  # (batch, lstm_size)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T08:51:01.759009Z",
          "iopub.execute_input": "2025-03-22T08:51:01.759329Z",
          "iopub.status.idle": "2025-03-22T08:51:01.765919Z",
          "shell.execute_reply.started": "2025-03-22T08:51:01.759304Z",
          "shell.execute_reply": "2025-03-22T08:51:01.764976Z"
        },
        "id": "nWzeyQDDKJ47"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultimodalNet(nn.Module):\n",
        "    def __init__(self, q_dim, i_dim, common_embedding_size, noutput, dropout=0.5):\n",
        "        super(MultimodalNet, self).__init__()\n",
        "\n",
        "        # Projections for question (q) and image (i)\n",
        "        self.q_proj = nn.Sequential(\n",
        "            nn.Linear(q_dim, common_embedding_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.i_proj = nn.Sequential(\n",
        "            nn.Linear(i_dim, common_embedding_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Output layer (following the same structure as the Lua code)\n",
        "        self.out = nn.Linear(common_embedding_size, noutput)\n",
        "\n",
        "    def forward(self, q, i):\n",
        "        q_proj = self.q_proj(q)\n",
        "        i_proj = self.i_proj(i)\n",
        "\n",
        "        q_proj = self.dropout(q_proj)\n",
        "        i_proj = self.dropout(i_proj)\n",
        "\n",
        "        # Element-wise multiplication of q_proj and i_proj (similar to CMulTable in Lua)\n",
        "        x = q_proj * i_proj\n",
        "\n",
        "        # Return the final output through the linear layer\n",
        "        output = self.out(x)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T08:51:04.041852Z",
          "iopub.execute_input": "2025-03-22T08:51:04.042186Z",
          "iopub.status.idle": "2025-03-22T08:51:04.048014Z",
          "shell.execute_reply.started": "2025-03-22T08:51:04.042161Z",
          "shell.execute_reply": "2025-03-22T08:51:04.047191Z"
        },
        "id": "ksZ70qUlKJ48"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, lstm_size, num_layers,\n",
        "                 image_feat_dim, common_embedding_size, noutput, dropout=0.5):\n",
        "        super(VQAModel, self).__init__()\n",
        "        self.embedding_net = QuestionEmbedding(vocab_size, embedding_size, dropout)\n",
        "        self.encoder = QuestionEncoder(embedding_size, lstm_size, num_layers, dropout)\n",
        "        self.multimodal = MultimodalNet(lstm_size, image_feat_dim, common_embedding_size, noutput, dropout)\n",
        "    def forward(self, question, lengths, image):\n",
        "        # question: (batch, seq_len)\n",
        "        # lengths: (batch,)\n",
        "        embeddings = self.embedding_net(question)            # (batch, seq_len, embedding_size)\n",
        "        q_encoded = self.encoder(embeddings, lengths)          # (batch, lstm_size)\n",
        "        scores = self.multimodal(q_encoded, image)             # (batch, noutput)\n",
        "        return scores"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T08:51:06.461431Z",
          "iopub.execute_input": "2025-03-22T08:51:06.461734Z",
          "iopub.status.idle": "2025-03-22T08:51:06.466782Z",
          "shell.execute_reply.started": "2025-03-22T08:51:06.46171Z",
          "shell.execute_reply": "2025-03-22T08:51:06.465806Z"
        },
        "id": "ETxkXtGTKJ49"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    # S·∫Øp x·∫øp batch theo ƒë·ªô d√†i c√¢u gi·∫£m d·∫ßn\n",
        "    batch.sort(key=lambda x: x['question_len'], reverse=True)\n",
        "\n",
        "    # Gh√©p c√°c gi√° tr·ªã th√†nh batch tensor\n",
        "    questions = torch.stack([item['question'] for item in batch], dim=0)\n",
        "    lengths = torch.tensor([item['question_len'] for item in batch])\n",
        "    images = torch.stack([item['image'] for item in batch], dim=0)\n",
        "    answers = torch.tensor([item['answer'] for item in batch])\n",
        "\n",
        "    return questions, lengths, images, answers\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T08:51:08.610406Z",
          "iopub.execute_input": "2025-03-22T08:51:08.610676Z",
          "iopub.status.idle": "2025-03-22T08:51:08.615744Z",
          "shell.execute_reply.started": "2025-03-22T08:51:08.610656Z",
          "shell.execute_reply": "2025-03-22T08:51:08.614778Z"
        },
        "id": "WItrTf1XKJ4-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(outputs, labels):\n",
        "    preds = torch.argmax(outputs, dim=1)\n",
        "    correct = (preds == labels).sum().item()\n",
        "    return correct / labels.size(0)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T08:55:24.929674Z",
          "iopub.execute_input": "2025-03-22T08:55:24.929981Z",
          "iopub.status.idle": "2025-03-22T08:55:24.934097Z",
          "shell.execute_reply.started": "2025-03-22T08:55:24.929958Z",
          "shell.execute_reply": "2025-03-22T08:55:24.933191Z"
        },
        "id": "XgsY2TxHKJ4_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from loguru import logger\n",
        "from torch.utils.tensorboard import SummaryWriter  # Import TensorBoard\n",
        "\n",
        "# C·∫•u h√¨nh Loguru ƒë·ªÉ d·ªÖ ƒë·ªçc log tr√™n Kaggle\n",
        "logger.remove()\n",
        "logger.add(lambda msg: tqdm.write(msg, end=\"\"), level=\"INFO\")\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Parameters\n",
        "    input_img_h5 = '/kaggle/input/meta-data/data_img.h5'\n",
        "    input_ques_h5 = '/kaggle/input/meta-data/cocoqa_data_prepro.h5'\n",
        "    input_json = '/kaggle/input/meta-data/cocoqa_data_prepro.json'\n",
        "    learning_rate = 5e-5\n",
        "    batch_size = 16\n",
        "    max_iters = 10000\n",
        "    input_encoding_size = 200\n",
        "    rnn_size = 512\n",
        "    rnn_layers = 2\n",
        "    common_embedding_size = 1024\n",
        "    noutput = 19\n",
        "    checkpoint_path = 'model/'\n",
        "    log_dir = 'logs/'  # Th∆∞ m·ª•c l∆∞u TensorBoard logs\n",
        "    seed = 42\n",
        "    patience = 5  # Early stopping patience\n",
        "\n",
        "    # Set random seed v√† device\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "\n",
        "    # T·∫°o th∆∞ m·ª•c l∆∞u model v√† logs n·∫øu ch∆∞a c√≥\n",
        "    os.makedirs(checkpoint_path, exist_ok=True)\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    # Kh·ªüi t·∫°o TensorBoard writer\n",
        "    writer = SummaryWriter(log_dir)\n",
        "\n",
        "    # Load dataset\n",
        "    logger.info(\"Loading dataset...\")\n",
        "    full_dataset = VQADataset(input_ques_h5, input_img_h5, input_json, split='train')\n",
        "    # Chia train/val (80/20)\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    logger.info(f\"Total dataset size: {len(full_dataset)}\")\n",
        "    logger.info(f\"Train dataset size: {train_size}\")\n",
        "    logger.info(f\"Validation dataset size: {val_size}\")\n",
        "\n",
        "    # DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # Kh·ªüi t·∫°o m√¥ h√¨nh\n",
        "    logger.info(\"Building model...\")\n",
        "    model = VQAModel(\n",
        "        vocab_size=full_dataset.vocab_size,\n",
        "        embedding_size=input_encoding_size,\n",
        "        lstm_size=rnn_size,\n",
        "        num_layers=rnn_layers,\n",
        "        image_feat_dim=4096,\n",
        "        common_embedding_size=common_embedding_size,\n",
        "        noutput=noutput,\n",
        "        dropout=0.5\n",
        "    ).to(device)\n",
        "\n",
        "    # Loss v√† optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # Early Stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    logger.info(\"üöÄ Starting training...\")\n",
        "\n",
        "    for iteration in range(max_iters):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        running_train_acc = 0.0\n",
        "\n",
        "        # Training Loop\n",
        "        for batch_idx, (questions, lengths, images, answers) in enumerate(tqdm(train_loader, desc=f\"Epoch {iteration+1}\")):\n",
        "            questions, lengths, images, answers = (questions.to(device),\n",
        "                                                   lengths.to(device),\n",
        "                                                   images.to(device),\n",
        "                                                   answers.to(device))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.amp.autocast(device_type='cuda'):\n",
        "                outputs = model(questions, lengths, images)\n",
        "                loss = criterion(outputs, answers)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            scaler.unscale_(optimizer)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            running_train_acc += compute_accuracy(outputs, answers)  # T√≠nh accuracy cho batch\n",
        "\n",
        "        # T√≠nh loss & accuracy trung b√¨nh\n",
        "        train_loss = running_train_loss / len(train_loader)\n",
        "        train_acc = running_train_acc / len(train_loader)\n",
        "        writer.add_scalar(\"Loss/Train\", train_loss, iteration)\n",
        "        writer.add_scalar(\"Accuracy/Train\", train_acc, iteration)\n",
        "\n",
        "        # Validation Loop\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        running_val_acc = 0.0\n",
        "        with torch.no_grad():\n",
        "            for questions, lengths, images, answers in val_loader:\n",
        "                questions, lengths, images, answers = (questions.to(device),\n",
        "                                                       lengths.to(device),\n",
        "                                                       images.to(device),\n",
        "                                                       answers.to(device))\n",
        "\n",
        "                with torch.amp.autocast(device_type='cuda'):\n",
        "                    outputs = model(questions, lengths, images)\n",
        "                    loss = criterion(outputs, answers)\n",
        "\n",
        "                running_val_loss += loss.item()\n",
        "                running_val_acc += compute_accuracy(outputs, answers)\n",
        "\n",
        "        # T√≠nh loss & accuracy trung b√¨nh cho validation\n",
        "        val_loss = running_val_loss / len(val_loader)\n",
        "        val_acc = running_val_acc / len(val_loader)\n",
        "        writer.add_scalar(\"Loss/Validation\", val_loss, iteration)\n",
        "        writer.add_scalar(\"Accuracy/Validation\", val_acc, iteration)\n",
        "\n",
        "        logger.info(f\"üìä Iter {iteration+1} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Ki·ªÉm tra Early Stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_path = os.path.join(checkpoint_path, 'best_model.pth')\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            logger.success(f\"‚úÖ New best model saved at {best_model_path} (Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            logger.warning(f\"‚è≥ Early stopping counter: {patience_counter}/{patience}\")\n",
        "            if patience_counter >= patience:\n",
        "                logger.error(\"‚õî Early stopping triggered! Training stopped.\")\n",
        "                break\n",
        "\n",
        "    # Save model cu·ªëi c√πng\n",
        "    final_ckpt = os.path.join(checkpoint_path, 'lstm_final.pth')\n",
        "    torch.save(model.state_dict(), final_ckpt)\n",
        "    logger.success(f\"üèÜ Final model saved to {final_ckpt}\")\n",
        "\n",
        "    # ƒê√≥ng TensorBoard writer\n",
        "    writer.close()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T05:32:18.741525Z",
          "iopub.execute_input": "2025-03-22T05:32:18.741839Z",
          "iopub.status.idle": "2025-03-22T05:32:18.763466Z",
          "shell.execute_reply.started": "2025-03-22T05:32:18.741815Z",
          "shell.execute_reply": "2025-03-22T05:32:18.762377Z"
        },
        "id": "AiJAlnmkKJ4_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T05:32:23.914572Z",
          "iopub.execute_input": "2025-03-22T05:32:23.914906Z",
          "iopub.status.idle": "2025-03-22T05:34:01.253734Z",
          "shell.execute_reply.started": "2025-03-22T05:32:23.91488Z",
          "shell.execute_reply": "2025-03-22T05:34:01.252827Z"
        },
        "id": "GDT2-6A1KJ5A"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternating Co-Attention"
      ],
      "metadata": {
        "id": "FZuA8mDBKJ5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as fn\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "class CoattentionNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Predicts an answer to a question about an image using Alternating Co-Attention only.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_embeddings, num_classes, embed_dim=512):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(num_embeddings, embed_dim)\n",
        "\n",
        "        self.unigram_conv = nn.Conv1d(embed_dim, embed_dim, 1, stride=1, padding=0)\n",
        "        self.bigram_conv  = nn.Conv1d(embed_dim, embed_dim, 2, stride=1, padding=1, dilation=2)\n",
        "        self.trigram_conv = nn.Conv1d(embed_dim, embed_dim, 3, stride=1, padding=2, dilation=2)\n",
        "        self.max_pool = nn.MaxPool2d((3, 1))\n",
        "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=embed_dim, num_layers=3, dropout=0.4)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        # Alternating Co-Attention parameters\n",
        "        self.W_x = nn.Linear(embed_dim, embed_dim)\n",
        "        self.W_g = nn.Linear(embed_dim, embed_dim)\n",
        "        self.w_hx = nn.Linear(embed_dim, 1)\n",
        "\n",
        "        # Fully connected layers for answer prediction\n",
        "        self.W_w = nn.Linear(embed_dim, embed_dim)\n",
        "        self.W_p = nn.Linear(embed_dim * 2, embed_dim)\n",
        "        self.W_s = nn.Linear(embed_dim * 2, embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, question, length, image):\n",
        "        # Embedding & convs\n",
        "        words = self.embed(question).permute(0, 2, 1)  # [B, D, T]\n",
        "\n",
        "        unigrams = torch.unsqueeze(self.tanh(self.unigram_conv(words)), 2)\n",
        "        bigrams  = torch.unsqueeze(self.tanh(self.bigram_conv(words)), 2)\n",
        "        trigrams = torch.unsqueeze(self.tanh(self.trigram_conv(words)), 2)\n",
        "\n",
        "        words = words.permute(0, 2, 1)  # [B, T, D] back for co-attn later\n",
        "\n",
        "        # Phrase-level features\n",
        "        phrase = torch.squeeze(self.max_pool(torch.cat((unigrams, bigrams, trigrams), 2)))  # [B, D, T]\n",
        "        phrase = phrase.permute(0, 2, 1)  # [B, T, D] for LSTM\n",
        "\n",
        "        # Pack LSTM input\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(phrase, length.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_output, _ = self.lstm(packed)\n",
        "        sentence, _ = pad_packed_sequence(packed_output, batch_first=True)  # [B, T, D]\n",
        "\n",
        "        image = image.view(image.shape[0], 512, -1)  # [B, 512, 196]\n",
        "\n",
        "        # Co-attention at each level\n",
        "        v_word, q_word = self.alternating_co_attention(words, image)\n",
        "        v_phrase, q_phrase = self.alternating_co_attention(phrase, image)\n",
        "        v_sent, q_sent = self.alternating_co_attention(sentence, image)\n",
        "\n",
        "        h_w = self.tanh(self.W_w(q_word + v_word))\n",
        "        h_p = self.tanh(self.W_p(torch.cat(((q_phrase + v_phrase), h_w), dim=1)))\n",
        "        h_s = self.tanh(self.W_s(torch.cat(((q_sent + v_sent), h_p), dim=1)))\n",
        "\n",
        "        logits = self.fc(h_s)\n",
        "        return logits\n",
        "\n",
        "\n",
        "    def alternating_co_attention(self, Q, V):\n",
        "        \"\"\"\n",
        "        Q: [B, T, D]\n",
        "        V: [B, 512, 196] ‚Üí needs to be transposed to [B, 196, 512]\n",
        "        \"\"\"\n",
        "        # Ensure V shape is compatible\n",
        "        V = V.permute(0, 2, 1)  # [B, 196, 512]\n",
        "\n",
        "        # Step 1: Attend to the question\n",
        "        H_q = self.tanh(self.W_x(Q))  # [B, T, D]\n",
        "        a_q = fn.softmax(self.w_hx(H_q), dim=1)  # [B, T, 1]\n",
        "        attended_q = torch.sum(a_q * Q, dim=1)   # [B, D]\n",
        "\n",
        "        # Step 2: Attend to the image using attended question\n",
        "        H_v = self.tanh(self.W_x(V) + self.W_g(attended_q).unsqueeze(1))  # [B, 196, D]\n",
        "        a_v = fn.softmax(self.w_hx(H_v), dim=1)  # [B, 196, 1]\n",
        "        attended_v = torch.sum(a_v * V, dim=1)   # [B, D]\n",
        "\n",
        "        # Step 3: Attend back to the question using attended image\n",
        "        H_q_final = self.tanh(self.W_x(Q) + self.W_g(attended_v).unsqueeze(1))  # [B, T, D]\n",
        "        a_q_final = fn.softmax(self.w_hx(H_q_final), dim=1)  # [B, T, 1]\n",
        "        final_q = torch.sum(a_q_final * Q, dim=1)            # [B, D]\n",
        "\n",
        "        return attended_v, final_q\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T10:22:41.547754Z",
          "iopub.execute_input": "2025-03-22T10:22:41.54813Z",
          "iopub.status.idle": "2025-03-22T10:22:41.560275Z",
          "shell.execute_reply.started": "2025-03-22T10:22:41.548104Z",
          "shell.execute_reply": "2025-03-22T10:22:41.559495Z"
        },
        "id": "o5humTscKJ5B"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import h5py\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "from loguru import logger\n",
        "from torch.utils.tensorboard import SummaryWriter  # TensorBoard logging\n",
        "\n",
        "# Logging configuration\n",
        "logger.remove()\n",
        "logger.add(lambda msg: tqdm.write(msg, end=\"\"), level=\"INFO\")\n",
        "\n",
        "def train():\n",
        "    # Parameters\n",
        "    input_img_h5 = '/kaggle/input/cnn-image/data_img_att.h5'\n",
        "    input_ques_h5 = '/kaggle/input/meta-data/cocoqa_data_prepro.h5'\n",
        "    input_json = '/kaggle/input/meta-data/cocoqa_data_prepro.json'\n",
        "    learning_rate = 1e-5\n",
        "    batch_size = 16\n",
        "    max_iters = 10000\n",
        "    embed_dim = 512\n",
        "    num_layers = 3\n",
        "    noutput = 19\n",
        "    checkpoint_path = 'model_att/'\n",
        "    log_dir = 'logs_att/'\n",
        "    seed = 42\n",
        "    patience = 5  # Early stopping patience\n",
        "\n",
        "    # Set random seed and device\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "\n",
        "    # Create directories if not exist\n",
        "    os.makedirs(checkpoint_path, exist_ok=True)\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    # TensorBoard writer\n",
        "    writer = SummaryWriter(log_dir)\n",
        "\n",
        "    # Load dataset\n",
        "    logger.info(\"Loading dataset...\")\n",
        "    full_dataset = VQADataset(input_ques_h5, input_img_h5, input_json, split='train')\n",
        "\n",
        "    # Train-validation split (80/20)\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    logger.info(f\"Total dataset: {len(full_dataset)}\")\n",
        "    logger.info(f\"Train dataset: {train_size}\")\n",
        "    logger.info(f\"Validation dataset: {val_size}\")\n",
        "\n",
        "    # DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    logger.info(\"Building Co-Attention model...\")\n",
        "    model = CoattentionNet(\n",
        "        num_embeddings=full_dataset.vocab_size,\n",
        "        num_classes=noutput,\n",
        "        embed_dim=embed_dim\n",
        "    ).to(device)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # Early Stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    logger.info(\"üöÄ Starting training...\")\n",
        "\n",
        "    for iteration in range(max_iters):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        running_train_acc = 0.0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {iteration+1}\"):\n",
        "            images, questions, answers, lengths = batch['image'].to(device), batch['question'].to(device), batch['answer'].to(device), batch['lengths'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast(device_type='cuda'):\n",
        "                outputs = model(questions, lengths, images)\n",
        "                loss = criterion(outputs, answers)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            scaler.unscale_(optimizer)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            running_train_acc += compute_accuracy(outputs, answers)\n",
        "\n",
        "        # Compute average loss & accuracy\n",
        "        train_loss = running_train_loss / len(train_loader)\n",
        "        train_acc = running_train_acc / len(train_loader)\n",
        "        writer.add_scalar(\"Loss/Train\", train_loss, iteration)\n",
        "        writer.add_scalar(\"Accuracy/Train\", train_acc, iteration)\n",
        "\n",
        "        # Validation Loop\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        running_val_acc = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                images, questions, answers = batch['image'].to(device), batch['question'].to(device), batch['answer'].to(device)\n",
        "\n",
        "                with autocast(device_type='cuda'):\n",
        "                    outputs = model(images, questions)\n",
        "                    loss = criterion(outputs, answers)\n",
        "\n",
        "                running_val_loss += loss.item()\n",
        "                running_val_acc += compute_accuracy(outputs, answers)\n",
        "\n",
        "        val_loss = running_val_loss / len(val_loader)\n",
        "        val_acc = running_val_acc / len(val_loader)\n",
        "        writer.add_scalar(\"Loss/Validation\", val_loss, iteration)\n",
        "        writer.add_scalar(\"Accuracy/Validation\", val_acc, iteration)\n",
        "\n",
        "        logger.info(f\"üìä Iter {iteration+1} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Early Stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_path = os.path.join(checkpoint_path, 'best_model.pth')\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            logger.success(f\"‚úÖ Best model saved at {best_model_path} (Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            logger.warning(f\"‚è≥ Early stopping counter: {patience_counter}/{patience}\")\n",
        "            if patience_counter >= patience:\n",
        "                logger.error(\"‚õî Early stopping triggered! Training stopped.\")\n",
        "                break\n",
        "\n",
        "    # Save final model\n",
        "    final_ckpt = os.path.join(checkpoint_path, 'final_coattention.pth')\n",
        "    torch.save(model.state_dict(), final_ckpt)\n",
        "    logger.success(f\"üèÜ Final model saved to {final_ckpt}\")\n",
        "\n",
        "    # Close TensorBoard writer\n",
        "    writer.close()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T05:55:21.698128Z",
          "iopub.execute_input": "2025-03-22T05:55:21.698499Z",
          "iopub.status.idle": "2025-03-22T05:55:21.719606Z",
          "shell.execute_reply.started": "2025-03-22T05:55:21.69847Z",
          "shell.execute_reply": "2025-03-22T05:55:21.718676Z"
        },
        "id": "-CCA8NV0KJ5B"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T05:55:37.302405Z",
          "iopub.execute_input": "2025-03-22T05:55:37.302759Z",
          "iopub.status.idle": "2025-03-22T06:00:15.266176Z",
          "shell.execute_reply.started": "2025-03-22T05:55:37.302729Z",
          "shell.execute_reply": "2025-03-22T06:00:15.265193Z"
        },
        "id": "gLCJgyI_KJ5C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorboard"
      ],
      "metadata": {
        "id": "ZreiarCdKJ5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "!ngrok authtoken 2ucmJoehETJPeF9zgDhTXhs7moj_4hGJMpihS1GcLN8AghBZC"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T05:34:11.863477Z",
          "iopub.execute_input": "2025-03-22T05:34:11.864212Z",
          "iopub.status.idle": "2025-03-22T05:34:17.143529Z",
          "shell.execute_reply.started": "2025-03-22T05:34:11.864179Z",
          "shell.execute_reply": "2025-03-22T05:34:17.14259Z"
        },
        "id": "UdOfzHouKJ5D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "import threading\n",
        "\n",
        "# Kill any existing TensorBoard processes\n",
        "os.system(\"pkill -f tensorboard\")\n",
        "\n",
        "# Start TensorBoard in a separate thread\n",
        "def run_tensorboard():\n",
        "    os.system(\"tensorboard --logdir=logs_att --host 0.0.0.0 --port 6006\")\n",
        "\n",
        "threading.Thread(target=run_tensorboard, daemon=True).start()\n",
        "\n",
        "# Create a public URL for TensorBoard\n",
        "tb_url = ngrok.connect(6006).public_url\n",
        "print(f\"Open TensorBoard: {tb_url}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T06:00:24.106511Z",
          "iopub.execute_input": "2025-03-22T06:00:24.106838Z",
          "iopub.status.idle": "2025-03-22T06:00:24.201012Z",
          "shell.execute_reply.started": "2025-03-22T06:00:24.10681Z",
          "shell.execute_reply": "2025-03-22T06:00:24.199394Z"
        },
        "id": "IDAiAqJVKJ5D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "c_5zwdX5KJ5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from loguru import logger\n",
        "\n",
        "def evaluate_model(model, dataset, batch_size=16, collate_fn=None,best_model_path = None):\n",
        "    \"\"\"\n",
        "    Evaluate any VQA model with top-k label filtering support.\n",
        "\n",
        "    Args:\n",
        "        model: Trained PyTorch model (MultimodalNet, CoattentionNet, etc.)\n",
        "        dataset: A torch.utils.data.Dataset instance for evaluation.\n",
        "        batch_size: Batch size for evaluation.\n",
        "        collate_fn: Optional collate_fn for DataLoader.\n",
        "    \"\"\"\n",
        "    logger.info(\"üîç Starting evaluation...\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # Load best checkpoint\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device, weights_only=True))\n",
        "    model.eval()\n",
        "\n",
        "    test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    running_test_loss = 0.0\n",
        "    running_test_acc = 0.0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    num_valid_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for questions, lengths, images, answers in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            questions, lengths, images, answers = (questions.to(device),\n",
        "                                                   lengths.to(device),\n",
        "                                                   images.to(device),\n",
        "                                                   answers.to(device))\n",
        "\n",
        "            valid_mask = answers != 19\n",
        "            if valid_mask.sum() == 0:\n",
        "                continue  # Skip if no valid samples\n",
        "\n",
        "            questions = questions[valid_mask]\n",
        "            lengths = lengths[valid_mask]\n",
        "            images = images[valid_mask]\n",
        "            answers = answers[valid_mask]\n",
        "\n",
        "            with torch.amp.autocast(device_type='cuda'):\n",
        "                outputs = model(questions, lengths, images)\n",
        "                loss = criterion(outputs, answers)\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "            all_labels.extend(answers.cpu().tolist())\n",
        "\n",
        "            running_test_loss += loss.item() * answers.size(0)  # multiply by batch size\n",
        "            running_test_acc += compute_accuracy(outputs, answers) * answers.size(0)\n",
        "            num_valid_samples += answers.size(0)\n",
        "\n",
        "    # Final average over valid samples\n",
        "    test_loss = running_test_loss / num_valid_samples\n",
        "    test_acc = running_test_acc / num_valid_samples\n",
        "    logger.info(f\"Number of valid test: {num_valid_samples} / {len(test_dataset)}\")\n",
        "    logger.info(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T10:04:34.592797Z",
          "iopub.execute_input": "2025-03-22T10:04:34.593169Z",
          "iopub.status.idle": "2025-03-22T10:04:34.601853Z",
          "shell.execute_reply.started": "2025-03-22T10:04:34.593144Z",
          "shell.execute_reply": "2025-03-22T10:04:34.600851Z"
        },
        "id": "yHlUB840KJ5E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T09:55:41.602432Z",
          "iopub.execute_input": "2025-03-22T09:55:41.602739Z",
          "iopub.status.idle": "2025-03-22T09:55:41.606716Z",
          "shell.execute_reply.started": "2025-03-22T09:55:41.602718Z",
          "shell.execute_reply": "2025-03-22T09:55:41.605793Z"
        },
        "id": "ocMYNVuuKJ5E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "input_img_h5 = '/kaggle/input/meta-data/data_img.h5'\n",
        "input_ques_h5 = '/kaggle/input/meta-data/cocoqa_data_prepro.h5'\n",
        "input_json = '/kaggle/input/meta-data/cocoqa_data_prepro.json'\n",
        "\n",
        "test_dataset = VQADataset(input_ques_h5, input_img_h5, input_json, split='test')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T09:53:24.578727Z",
          "iopub.execute_input": "2025-03-22T09:53:24.579078Z",
          "iopub.status.idle": "2025-03-22T09:53:24.735287Z",
          "shell.execute_reply.started": "2025-03-22T09:53:24.579048Z",
          "shell.execute_reply": "2025-03-22T09:53:24.734282Z"
        },
        "id": "0TRPMr2AKJ5F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Model Eval"
      ],
      "metadata": {
        "id": "pJRFlMB9KJ5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_encoding_size = 200\n",
        "rnn_size = 512\n",
        "rnn_layers = 2\n",
        "common_embedding_size = 1024\n",
        "noutput=19\n",
        "\n",
        "# Load model & set to eval mode\n",
        "model = VQAModel(\n",
        "    vocab_size=test_dataset.vocab_size,\n",
        "    embedding_size=input_encoding_size,\n",
        "    lstm_size=rnn_size,\n",
        "    num_layers=rnn_layers,\n",
        "    image_feat_dim=4096,\n",
        "    common_embedding_size=common_embedding_size,\n",
        "    noutput=noutput,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "best_model_path = '/kaggle/input/vqa-best/pytorch/default/1/baseline_model.pth'\n",
        "evaluate_model(\n",
        "    model=model,\n",
        "    dataset=test_dataset,\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    best_model_path = best_model_path\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T10:04:57.432186Z",
          "iopub.execute_input": "2025-03-22T10:04:57.432502Z",
          "iopub.status.idle": "2025-03-22T10:04:57.94317Z",
          "shell.execute_reply.started": "2025-03-22T10:04:57.432474Z",
          "shell.execute_reply": "2025-03-22T10:04:57.942111Z"
        },
        "id": "O_OXOzE-KJ5F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Model Eval"
      ],
      "metadata": {
        "id": "sTCGGr8HKJ5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 512\n",
        "model = CoattentionNet(\n",
        "        num_embeddings=test_dataset.vocab_size,\n",
        "        num_classes=noutput,\n",
        "        embed_dim=embed_dim\n",
        "    ).to(device)\n",
        "\n",
        "best_model_path = '/kaggle/input/vqa-best/pytorch/default/1/attention_model.pth'\n",
        "evaluate_model(\n",
        "    model=model,\n",
        "    dataset=test_dataset,\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    best_model_path = best_model_path\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-22T10:22:52.044698Z",
          "iopub.execute_input": "2025-03-22T10:22:52.045013Z",
          "iopub.status.idle": "2025-03-22T10:22:53.549886Z",
          "shell.execute_reply.started": "2025-03-22T10:22:52.044989Z",
          "shell.execute_reply": "2025-03-22T10:22:53.549125Z"
        },
        "id": "U3pO3D3AKJ5F"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}